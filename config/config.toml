# Default LLM configuration (DeepSeek via Hugging Face)
[llm]
api_type = "huggingface_deepseek"
model = "deepseek-ai/DeepSeek-R1-0528"
base_url = "https://api-inference.huggingface.co/models"
api_key = ""
max_tokens = 4096
temperature = 0.7
timeout = 120
retry_count = 3
streaming_supported = true

# Vision model configuration using DeepSeek
[llm.vision]
api_type = "huggingface_deepseek"
model = "deepseek-ai/DeepSeek-R1-0528"
base_url = "https://api-inference.huggingface.co/models"
api_key = ""
max_tokens = 4096
temperature = 0.7

# OpenAI configuration (alternative provider)
[llm.openai]
api_type = "openai"
model = "gpt-4o"
base_url = "https://api.openai.com/v1"
api_key = ""
max_tokens = 4096
temperature = 0.7

# Azure OpenAI configuration (alternative provider)
[llm.azure]
api_type = "azure"
model = "gpt-4"
base_url = ""
api_key = ""
api_version = "2024-02-01"
max_tokens = 4096
temperature = 0.7

# Anthropic configuration (alternative provider)
[llm.anthropic]
api_type = "anthropic"
model = "claude-3-sonnet-20240229"
base_url = "https://api.anthropic.com/v1"
api_key = ""
max_tokens = 4096
temperature = 0.7

# DeepSeek configuration (explicit section)
[llm.huggingface_deepseek]
api_type = "huggingface_deepseek"
model = "deepseek-ai/DeepSeek-R1-0528"
base_url = "https://api-inference.huggingface.co/models"
api_key = ""
max_tokens = 4096
temperature = 0.7
timeout = 120
retry_count = 3

# Browser configuration
[browser]
headless = true
disable_security = true
extra_chromium_args = []
max_content_length = 2000

# Search configuration
[search]
engine = "Google"
fallback_engines = ["DuckDuckGo", "Bing"]
retry_delay = 60
max_retries = 3
lang = "en"
country = "us"

# Sandbox configuration
[sandbox]
use_sandbox = false
image = "python:3.12-slim"
work_dir = "/workspace"
memory_limit = "512m"
cpu_limit = 1.0
timeout = 300
network_enabled = false

# MCP configuration
[mcp]
server_reference = "app.mcp.server"
